tip)mean의 axis 매개변수 값은 0이면 종방향(한 열씩 평균냄), 1이면 횡방향(한 행씩 평균냄)으로 평균을 낸다.
(아무 값도 지정 안 하면 그냥 수치적으로만 평균을 낸다.)

tip)np.argsort : 직접적으로 값을 기준으로 원본을 sort하지 않고, 작은 것부터 큰 순서대로 원본의 인덱스 값을 나열해놓은 행렬을 반환한다.

tip)fig,axs=plt.subplots(10,10,figsize=(10,10)) : 이러면 axs는 10x10 배열이 되고, 각각이 subplot이 된다.

tip)npy파일은 넘파이로 저장한 파일이다. 이런 파일은 넘파이의 load 함수로 불러올 수 있다.

tip)3차원 넘파이 배열은 이미지로, (300,100,100)인 넘파이 배열은 100x100 픽셀 사진 300장이다
(0과 1로 표현된 경우, 흑백사진임.)

tip)matplot으로 그림을 그려볼려면 imshow를 사용한다. (2차원 넘파이 배열을 넘겨줘야한다.)

ㅁ이미지 학습은 이전의 정형화된 데이터를 다룰 때와는 달리 비지도 학습을 사용한다.
문제집을 주되, 정답지를 주지 않고 힌트를 주지 않음.

ㅁ비지도 학습

비지도 학습을 사용하는 이유는 여러 가지가 있다.

1.비지도 학습에 쓰이는 이미지의 경우 하나하나 라벨링을 해주기는 어렵다. (데이터 자체에 target 값이 있는 건 아니니까)
따라서, 힌트만 준다. 예를 들면 두 가지 동물이 있다 정도로만.

2.스스로 발전하는 인공지능을 만들 수가 있다.
예를 들면 사람 사진을 그리게 학습하는 AI와 사람이 그린건지 컴퓨터가 그린건지 판단하게 하는 AI를 만드는 것이다.
즉, 서로서로가 문제집과 정답지가 되도록 만드는 것이다. 이러면 서로를 속이기 위해 노력해야한다. 
(ex This Person doesn't exist) =>이걸 GAN이라고 함.

-컴퓨터는 이미지를 행렬을 한 행 한 행 읽어서 파악한다. 따라서 하나의 사진을 일자로 펼쳐줘야한다.
(100x100이면 10000이 되면 한 행으로 펼쳐진다.)
=>reshape로 바꿔준다. ( reshape(이미지장수, nxn) )



ㅁ비지도 학습 모델 :

ㅇKMeans - 평균을 이용함. 
각각 데이터의 평균값을 내서 기억한 다음, 들어온 데이터의 평균값을 내어 가장 가까운 값의 데이터로 분류하는 방식임.

*일단 직접 해보면서 어떻게 이 모델이 굴러가는지 이해해보자.
-평균을 이용하더라도 이미지 한 장 한장의 평균, 그리고 이미지 한 픽셀 한 픽셀의 평균 두 개의 평균을 이용하여 잘 찾아낼 수 있다.
-사과, 파인애플, 바나나 사진을 예로 들겠다.
무작위 과일 사진 300장이 있을 때, (과일 사진 - 사과 픽셀값 평균)을 해서 나온 행렬에서, 픽셀값 평균(한 이미지의 값) 합이 0에 가까운 것이 사과 사진일 것이다. 
=>따라서, (과일 사진-사과 픽셀값 평균) 행렬의 픽셀값 평균이 합이 1~100등인 것을 찾으면 사과가 된다는 말이다

-어떤 식으로 이미지 분류를 하는지는 KMeans gif를 참고하자. (평균을 내며 자신의 군집으로 이동하는 거 같긴 한데, 잘 모르겠다.)

*라이브러리를 이용하자.
이 모델을 사용할 수 있는 라이브러리 호출법은 다음과 같다.
from sklearn.cluseter import KMeans

그리고 학습을 시키려면 다음과 같이 한다.
km=KMeans(n_clusters=3)
km.fit(훈련이미지)

n_clusters=는 이미지 종류(군집)의 갯수다.
근데,  개발하는 사람이 넣은 이미지가 몇 종류로 이뤄져있는지 항상 알 수는 없다. (군집이 몇 개 있는지 모르는 경우도 많다는 말이다.)
그래서 어떻게 아느냐면, 이너셔를 이용한다.

이너셔(inertia) : 각 군집의 중심으로부터 각 군집의 요소(?)와의 거리의 제곱을 더하고, 각 군집의 그런 연산 값을 총합을 낸 것.
(A, B 군집이 있다면, A 군집의 중심-요소간 거리의 제곱 + B 군집의 중심-요소간 거리의 제곱이 이너셔라는 말이다.)
그러면 n개를 시뮬레이션을 돌려서 이너셔의 값 변화를 보는 것이다. (군집을 만들수록 작아지기야 하겠지만, 어느 시점에서 엄청 작아지는 시점이 있을 것이다. 그 시점을 찾는다.) 
이너셔는 km.inertia_ 이렇게 볼 수 있다.
plot으로 그려 봤을 때 기울기가 가장 급격한 부분을 관찰하면 된다.

훈련 이미지의 라벨링을 보려면 이렇게 한다.
km.labels_

정형화된 데이터를 집어넣어서 군집이 몇 개인지 살펴보더라도 군집의 분류 타입은 다를 지어정 군집 형성이 비슷하게 잘 되는 것을 볼 수가 있다.

*머신러닝 이미지 조작 팁
실습에 쓴 데이터는 100x100으로, 본인 유튜브 프로필에나 쓸까말까 할 정도로 작은 사진이다. 
현대의 이미지는 픽셀로만 따져도 최소한 200만 픽셀이 넘어간다. 이걸 그대로 학습시키라고 던지면 컴퓨터가 많이 아파할 것이다.
그래서 머신 러닝을 빠르게 위한 방법을 살펴보자.

1. 이미지 사이즈를 줄이기 : 데이터 설명력이 줄어든다는 단점이 존재.
2. 주성분 분석(데이터의 특징을 분석) 
=>예를 들자면, 데이터의 방향을 그래프로 그리고, 데이터의 분포정도를 또 하나의 직선으로 나타낸다. (예를 들자면 그런 것이다. 특징은 여러가지로 찾을것이다.)
┴ 모양 예시가 좀 나쁘지만, 이런 식으로. (ㅡ가 방향이고, ㅣ가 분포 정도다) 그리고 여기서 방향은 1등짜리 주성분이고 분포정도는 2등짜리 주성분이다.
(주성분은 등수를 매길 수가 있다.) => 데이터 픽셀이 하나에 200만 개인데 n개 주성분으로 하나를 표현할 수 있으면 이득이기는 하다.

ㅇ주성분 분석
라이브러리는 다음과 같다. from sklearn.decomposition import PCA

주성분의 갯수를 지정하는 건 다음과 같다.
pca=PCA(n_components=)

그리고 이미지갯수 x 주성분갯수 의 행렬을 반환하려면 다음과 같이 쓰면 된다.
fruits_pca=pca.fit_transform(fruits2)

강의의 예제로 쓴 fruits는 본래 300x10000의 행렬이었지만 fruits_pca는 300x50의 행렬이 된다.
다만, 이렇게 픽셀을 압축하면 데이터 설명력이 감소한다. (당연히 데이터가 손실되니까.)

주성분들의 설명력을 보기 위해서는 다음 속성으로 확인해볼 수 있다.
pca.explained_variance_ratio_
순서대로 1등 성분 설명력, 2등 성분 설명력... 이렇게 나타내준다

그리고 데이터의 설명력을 보려면 다음과 같이 알 수가 있다.
sum(pca.explained_variance_ratio_)
강의의 데이터를 쓴 결과 주성분 50개면 92% 정도 원본을 설명할 수 있다고 나온다. 

그리고 그래프를 그려보면...
plt.plot(pca.explained_variance_ratio_) 이렇게 그려볼 수가 있다.
적분을 해보면 대략 몇 등 주성분까지가 유의미한 정보인지도 알 수가 있다.

ㅁ인공신경망(딥러닝)

미분을 이용함.

일단 파이썬에서 인공 신경망을 다루기 위해서는 numpy를 잘 알아야 한다.

ㅇnumpy

numpy.array로 만들어지는 것은 넘파이 배열이다. (행렬과 유사하기는 하지만, 행렬은 아님.)

arr=numpy.array([1,2,3]) 이렇게 arr에 넘파이 배열을 할당했다고 하자.

-arr.dtype 
배열의 타입을 알 수가 있다.
다만, 정수 원소가 수천만 개인데 실수 원소가 하나라도 있으면 float 타입으로 판정한다.  

-n차원 배열도 만들 수 있다.
arr1=numpy.array([ [1,2] ,[3,4] ])
arr2=numpy.array([ [0,1] , [2,3] ])

-동일한 형상(shape)을 가진 넘파이 배열을 더하면 같은 위치에 있는 것을 더한다.
arr1+arr2 #[ [1,3] , [5,7] ]

동일한 형상(shape)을 가진 넘파이 배열을 곱하면 같은 위치에 있는 것을 곱한다. (행렬곱이 아니다.)
arr1*arr2 #[ [0,2] , [6,12] ]

-다른 형상끼리 연산하는 것을 브로드캐스트라고 한다. 다음과 같은 넘파이 배열이 있다고 하자.
arr1=numpy.array([ [1,2] ,[3,4] ])
arr2=numpy.array([10,20])

그러면 arr1 * arr2를 했을 때 다음과 같이 연산한다. (다른 대수 연산도 마찬가지이다.)
[[10,20],[30,80]]

-원소의 접근은 파이썬의 리스트의 접근과 같이 [] 연산자로 할 수 있다.

그리고 n차원 배열을 1차원 배열로 펴버릴 수가 있는 함수가 있다.
arr1.flatten()

여러 개 원소에 접근할 수도 있다.

arr2[[0,2,4]] ([] 안에 넘파이 배열을 넣어도 된다.)

arr2 > n 이런 식으로 조건을 입력하면 bool 값 넘파이 배열이 반환된다.

이 bool값 넘파이 배열을 이용해 조건에 맞는 원소만 접근할 수도 있다.
arr2[arr2>15]

(데이터프레임의 그것과 유사함)

-사용할 수 있는 대표적인 함수는 다음과 같다.

numpy.argmax(넘파이배열, axis=) : (각 행/열에서) 가장 큰 값을 가진 index를 반환한다.
axis=0 : 종방향, axis=1: 횡방향. 어느쪽도 디폴트가 아님에 유의

numpy.sum(넘파이배열,axis=) : 디폴트값은 모든 항의 합.

numpy.sin(넘파이배열), numpy.cos(넘파이배열) : 사인, 코사인에 배열에 있는 값을 대입하여 똑같은 크기/형상의 배열로 반환.

numpy.arange(실수,실수,실수) : range와 다른 점은 실수 단위를 적을 수 있다는 점이다.

arr.astype(...) : 내부 원소의 값을 해당 타입으로 캐스팅한다. 

-행렬곱을 하려면...
np.dot(arr1,arr2) 
(지켜줘야 하는 것은 행렬곱에서 지켜야하는 원칙과 같다. 앞의 것의 열 갯수와 뒤의 것의 행 갯수를 맞춘다.)

tip)plt.plot(x값,y값, label=,linestyle=)
label로 그래프를 구분하도록 할 수 있다.(외적인 변화는 없음) linestyle은 점선, 동그라미선 등 다양한 선 스타일을 지정할 수 있다.
plt.legend([라벨이름들..])는 분류를 할 수 있는 box를 띄워준다. 라벨 이름을 적으면 자동으로 label도 생성해준다.

ㅇ퍼셉트론 (동영상으로 복습하도록 하자)

인공 신경망(딥러닝)의 기원이 되는 알고리즘이다.
다수의 신호를 입력으로 받아서 하나의 신호를 출력하며, 신호는 흐르거나 흐르지 않는다(0 or 1) 두 가지 값 중 하나를 가짐.

강사의 설명은 프로그래밍에서 자주 말하는 가중치가 있는 그래프 형태로 퍼셉트론을 표현했다. (물론 일반적인 그래프의 활용방법과는 매우 차이가 있다.)
두 개의 node 값이 각각 x1, x2라고 하고, 각각 edge의 가중치 값을 w1, w2라고 하자. 또, 각각의 edge가 같은 하나의 노드를 가리키는 형태라고 하자. (체리 모양을 상상하라.)
이 때...최종 도착지로 이어진 node에 도달하는 값은 w1*x1+w2*x2 이고, 이 값이 일정 값(임계값)을 넘으면 1, 넘지 않으면 0을 보낸다.
식으로 보면...
y = 0 (w1x1 + w2x2 <= Θ)
 or  1 (w1x1 + w1x1 > Θ)

이 퍼셉트론에서의 식을 이용하여 AND 게이트, OR 게이트 등의 논리 게이트를 만들 수도 있다. (Θ와 w1, w2를 잘 설정하면 된다.)
다만 이 식만으로는 XOR를 구현하기가 쉽지 않다.

이유를 설명하길, w1x1 + w2x2는 사실상 선형 모델(직선)인데 직선으로 평면을 나누어서 (0,0),(1,1) 과 (1,0),(0,1)로 나누기는 불가능하다.
대신, 여러 개의 게이트를 이용하면 된다.

ㅇ활성화 함수

여러 개의 게이트를 사용한다는 것은...

-인공 신경망은 입력층, 은닉층, 출력층으로 나뉜다. 은닉층은 복잡한 연산을 하는 층이다.

하나의 층이 아니라 여러 개의 층을 사용하겠다 이것이다. (함수를 합성하겠다.)

여기서 이제 위 식을 이렇게 고쳐보자 (Θ를 왼쪽으로 이항한다. 근데 Θ는 어차피 뭔지 모르는 값이므로 그냥 뭉뚱 그려 a라고 포현해도 무관하다.)

y = 0 (w1x1 + w2x2 + a <= 0)
 or  1 (w1x1 + w2x2 + a > 0)

그리고 w1x1 + w2x2 + a를 함수에 집어넣는다. 그러면...

y = 0( h(w1x1 + w2x2 + a) <= 0) 
  or 1( h(w1x1 + w2x2 + a) > 0)

이런 식일 것이다. 이런 h(x)를 활성화 함수라고 한다. 퍼셉트론 식에서 보듯이, 퍼셉트론의 활성화함수 h(x)는 계단함수다.

활성화 함수 : 입력 신호(퍼셉트론 식으로 더한 것을 말하는 듯?)를 출력 신호로 변환하는 함수.

여러 개의 게이트를 쓴다는 말은 아래 글을 보자

-인공신경망은 입력층, 은닉층, 출력층으로 나뉜다. 은닉층은 복잡한 연산을 담당한다.

함수를 여러 번 사용한다는 말이다. f(x)에서 나온 결과를 g(x)에 집어넣고... => 함수의 합성이다.

이런 퍼셉트론 식은 보면 알겠지만 선형 함수이고, 이 경우 모든 것을 하나의 식으로 표현할 수는 없다. 

여러 개의 식을 사용한다고 해도, 결국 직선 n개에 불과하다. 이런 방법 말고, 다른 방법을 찾아보자.

퍼셉트론의 활성화 함수는 계단함수다.

인공신경망의 활성화 함수는 시그모이드 함수이다.

인공신경망이 시그모이드 함수를 활성화 함수로 사용하는 이유는 접선의 기울기를 알 수가 있어서 방향성을 알기 편하기 때문이다.
(반대로 퍼셉트론의 활성화 함수인 계단함수는 접선의 기울기가 의미가 없다. 그냥 적당히 값을 조정할 수 밖에 없는데, 어느 방향으로 조정될지는 보장이 안된다.)

계단함수나 시그모이드 함수나 비선형 함수이다. 또, 활성화 함수는 무조건 비선형 함수여야 한다.
(여러 개의 선형식은 합성해봐야 c^nx에 불과하다. 이는 그냥 bx로 표현해도 다를 건 없다.)

=>궁금하면 퍼셉트론, 활성화함수, 인경신공망을 좀 더 검색해보자.

그 외 많이 쓰이는 활성화 함수에는 렐루(relu) 함수가 있다. 0과 비교하여 더 큰 값을 반환하는 함수이다.

tip)인공 신경망 연산에는 행렬의 곱이 필요하다.

-소프트 맥스 : 출력층에서 사용되는 활성화 함수 (최종적으로 보는 출력값이 나오는 함수)

식은 다음과 같다. e^(출력값)  / ∑(각항의 e^(출력값)  (각 행렬의 항마다 적용)

근데 식을 보면 알겠지만 지수가 너무 크면 소프트 맥스의 결과값을 볼 수가 없다. 그럴 때는...
지수에 넣기 전에 출력값을 이렇게 한다.
a = a - numpy.max(a)

이러면 계산이 되긴 되니까 확률 보기가 편하다.

ㅇ경사하강법
 
어떤 값들을(이 경우 학생 4명의 성적과 시간의 상관관계라고 하자) 대표하는 직선의 식을 y_=ax+b라고 하자.
그리고 오차 e를 e=∑(y_-y)^2 라고 하자. 그러면 e=∑(ax+b-y)^2 의 꼴이 된다.
e는 뭐 시그마야 어쨌든 다항함수의 꼴이 된다.

오차는 0에 가까워야 좋은 것이다. a,b를 조절하면서 찾아보자.(학습 시점에서 x,y는 고정된 값임에 유의한다. 변수는 기울기와 절편뿐이다.)
e는 다항함수이므로 어떤 점 k에 대해 e`(k)=a`가 존재할 것이다.
e`(k)가 0이 아니라면, a를 k-e`(k)로 바꾸면서 계속하여 경사를 하강시켜 나간다.
(e`(k)가 0보다 크면 a는 접선의 기울기가 0에 가까운 방향으로 감소할 것이고(실제 a`도 감소할 것이다.), 
e`(k)가 0보다 작으면 a는 접선의 기울기가 0에 가까운 방향으로 증가할 것이다(실제 a`도 증가할 것이다.)
어느 쪽이건 a-e`(k) 자체에는 이론상으로 문제는 없다.)

다만 e`(a)의 절댓값이 매우 큰 경우 a-e`(k)를 하면 a가 접선의 기울기가 0인 곳으로 내려가는 게 아니라,
a가 위로 올라갈 수도 있다. 따라서 이런 걸 방지하기 위해...

a=a-e`(k)x0.1 이런 식으로 해준다. 이 때 미분계수에 곱해주는 값을 학습률이라고 한다.

학습률이 너무 크면 발산하고, 너무 작으면 매우 느리게 경사가 하강한다.

tmi)e 함수가 다항함수의 합이기 때문에, 보통은 학습률이 없으면 위로 발산하게 된다.

이 때 e의 a에 대한 편미분=∑2(ax+b-y)*x = ∑2ex , e의 b에 대한 편미분 =∑2(ax+b-y)*1=∑2e 이다.
(x, y에 대한 미분이 아니라, a, b에 대한 미분임에 주의하자. 그래서 x의 차수가 올라가건 말건 상관없이 a,b 등 계수와 절편에 대해서는 2차식인 셈.)

de(a,b) = (a편미분)da + (b편미분)db (변수가 늘어나더라도 늘어난 변수만 추가해주면 된다.)

de(a,b)는 e`(k) 이다. ( k는 a가 얼마고 b가 얼마고를 내포하고 있는 좌표값이다.)

다만 공부 시간-시간 관계를 추정한 그래프는 매우 오차가 큰데, 이는 여러 요인이 있지만 주요한 요인은 공부 시간만이 성적에 영향을 미치는 요인이 아니기 때문이다.
그래서 요인이 많을 수록 도움이 된다. (직선을 하나 더 그을 수 있으면 평면을 그릴 수도 있으니까)

요인이 늘어난 경우 y= a1x1 + a2x2 + b 를 해서 직선을 하나 더 긋는다거나.
y= ax^2 + bx + c 이렇게 n차 함수꼴로 할 수가 있다. (삼각함수는 직선 몇 개 더 긋는다고 될 수준이 아니다.)

직접 경사하강법을 사용하는 예는 230202를 참고.

ㅇ인공신경망 라이브러리

-from tensorflow import keras

-keras에는 연습용 데이터셋이 존재한다. (keras.datasets로 찾아보자.) 불러올 때는 이런 식으로 부른다.
(train_input,train_target),(test_input,test_target)=keras.datasets.fashion_mnist.load_data()

-모델의 생성자는 다음과 같다.
model = keras.Sequential() 
강사가 말하길, 신경망을 설계하기 위한 도화지를 그려주는 느낌이라고 한다.

인공 신경망의 층은 이렇게 추가할 수가 있다.

-model.add(keras.layers.Dense(10,activation=,input_shape=))
keras.layers.Dense는 층으로, 노드 덩어리를 말한다.

-keras.layers.Dense(10,activation=,input_shape=)
첫번째 인자는 출력층의 노드 개수, activation은 해당 층의 활성화함수, input_shape는 입력층의 노드 개수가 된다.

(출력층의 활성화 함수는 softmax임을 기억하자. 은닉층이야 용도에 따라 무엇을 사용해도 무관하지만, 강의에선 시그모이드를 쓴다.)

model.summary() 함수는 이 인공신경망이 경사하강법을 통해 몇 개 변수의 기울기를 구해야하는 지 알려준다.
(예제에서는 784x10 + 10 이라 7850이라고 한다.)

-학습 방법을 알려주는 것은 다음과 같다.
model.compile(loss='categorical_crossentropy',metrics='accuracy')
loss는 오차를 어떻게 할 것인지, metrics는 학습의 방향성을 어떻게 할 것인지 결정한다.
tip)
accuracy로 metrics를 설정하면 정확도를 늘리는 방향으로만 학습한다. (가능성을 정확히 따지지는 않는다.)
예를 들어 떡상 데이터 10만개와 떡락 데이터 1000개를 주고 학습시키면 이 학습 데이터로만 따졌을 때 무조건 떡상이라고만 대답하는 게 정확도가 높다.
그래서 뭘 넣든 그렇게 대답할 확률이 높아진다. 학습을 잘 시키려면..
1. 떡상 데이터 : 떡락 데이터의 비율을 맞춰준다.
2. 떡락 데이터를 더 넣는다.
사진의 경우 데이터를 더 늘리는 방법은 사진을 조금 훼손하는 것이다. (늘린다거나, 줄인다거나, 흐리게 한다거나...)
데이터프레임은 그러기가 쉽지는 않다.

tip) loss에 'sparse_categorical_crossentropy'를 할당하면, 원핫 인코딩을 자동으로 한다.

-target도 원핫 인코딩과 같은 방법을 통해 출력층의 노드에 하나하나 넣을 수 있게 바꿔줘야 한다.
ex) 9를 [0,0,0,0,0,0,0,0,0,1] 이렇게 표현한다. 그냥 9를 넣으면 이상한 출력값을 내보낼 수도 있다.

train_target=keras.utils.to_categorical(train_target)
test_target=keras.utils.to_categorical(test_target)

-crossentropy(교차 엔트로피) : 분류 모델의 오차 계산 방법 중 하나.
(회귀 모델의 평균 절대 오차, 평균 제곱 오차를 생각해보면 된다.)

크로스 엔트로피는 log 함수를 사용하기 때문에, 들어가는 인자에 적당히 작은 양수를 더해줄 필요가 있다.
따라서...
numpy.sum(numpy.log(result+0.001)*target)

이런 식으로 계산한다는 말이다. 목적은 저 위 식의 값이 최대한 1에 가깝도록 하는 것이다.
(result가 target에 가까운 추측을 해야 1에 가까워진다.)

-학습 시키려면 
hist=model.fit(훈련데이터,훈련데이터의 답,epochs=30,validation_data=(검사용데이터, 검사용답 )) 으로 하면 된다. (반환 값은 변수에 집어넣는 것이 좋다고 한다.)
epochs는 학습을 반복하는 횟수이다. (모델을 여러 개 만드는 것이라 생각하면 된다.)
validation_data에 데이터와 값을 튜플로 지정하면 일종의 쪽지시험을 수행한다. 과대적합이 되었나 안되었나 알기 위해서는 정확도보단 이것을 활용하는 것이 좋다.

-만약 fit 함수의 결과를 반환 받았다면, hist.histroy['loss']를 통해 오차의 변화를 볼 수 있다.
(hist의 history는 딕셔너리다.)

-만약 validation_data를 설정했다면, hist.history['val_loss']를 통해 검사 데이터의 오차를 볼 수가 있다.

tip) 인공신경망에서 은닉층이 없다면 딥러닝이라고 부르지는 않는다. 은닉층이 있다면 딥러닝.

-다만 학습을 여러 번 시키면 과대적합이 발생하여 훈련데이터에만 적합한 결과를 내놓는다.
그래서 훈련 데이터 맞춤 모델이 되도록 하지 않기 위해서 '적절한 방해'가 필요하다.

그 학습을 방해하는 방법으로 레이어에 dropout을 추가하는 방법이 있다.
dropout이란 몇 개의 node를 출력층에 가지 못하게 꺼버리는 방법을 말한다.
model.add(keras.layers.Dropout(0.3)) 
인자는 비율이다. 주의할 점은 50%를 넘기지 않아야 한다는 점이다.
또 주의할 점은 dropout은 순서가 중요하다는 점이다. (꺼버릴 레이어를 추가한 아랫줄에 작성해야한다.)

-epochs의 단점은 결과물이 가장 마지막에 학습을 모델로 선정이 된다는 것이다. val_loss를 보면 가장 좋은 모델이 저장되고 있지는 않음을 알 수 있다.

그래서 텐서플로우에는 다음과 같은 함수를 제공한다.

es=keras.callbacks.EarlyStopping(patience=10,restore_best_weights=True)
patience회 이상 연속으로 개선되지 않는다면 학습을 강제종료 하도록 한다.
restore_best_weights를 True로 하면 강제 종료 시점에서, patience 회분만큼의 생성된 모델은 무효로 한다.
(보통 patience는 15~30을 넣어준다.)

이 es는 이렇게 넣어주면 된다.

hist=model1.fit(훈련데이터,훈련데이터답,epochs=,validation_data=(검사데이터,검사답),callbacks=[callback들])
callbacks의 []에 넣어준다.

그리고 저 EarlyStopping의 기준은 validation_data로 인한 val_loss다.

기존 사이킷런의 predict함수의 역할을 하는 것은 model.evaluate(테스트데이터, 테스트답) 함수다.

-tip으로, 은닉층의 활성화 함수는 relu 함수를 사용하는 것이 좋다.
why?)
직접 손으로 구현했던 경사하강법에서 보면, a 기울기의 값의 전달이 100% 이뤄진다. a-a` 가 된 a 값은 다음 경사하강 과정에 그대로 전달된다 그 말이다.
하지만 은닉층이 많아지는 딥러닝에서는 한 층에서 인접한 한층으로만 값의 전달이 가능하다.
근데 문제는, 위의 경사하강법 예시는 값을 그대로 미분해서 줬지만, 딥러닝에서는 값을 활성화 함수에 집어넣고 그 상태로 미분한다는 점에 있다.
시그모이드 함수의 경우 미분하면 값이 30%가 된다고 한다. 근데 은닉층이 많으면 다음 학습 회차에 들어가는 값은 없는 데이터에 가깝다.
하지만 relu 함수의 경우 미분하면 1로 들어간다. 하지만 마지막 출력층에서는 softmax, 즉 시그모이드와 유사한 함수를 사용하므로 30%만큼만 다음에 전달할 수 있다.
그래도 없는 데이터를 다음 회차에 집어넣는 것보다는 낫다. 따라서, relu함수를 은닉층의 활성화함수로 사용하는 것이 좋다.

ㅇ인공신경망 라이브러리-회귀모델

분류할 필요가 있다면 활성화 함수를 이용하지만, 회귀 모델이면 계산한 값 그 자체가 필요하므로 활성화 함수는 필요가 없다.

그리고 compile도 회귀에 맞게 써줘야 한다.

model.compile(loss='mse',metrics='mae')
(metrics가 mae인 이유는 보기 편하기 위해서다.)