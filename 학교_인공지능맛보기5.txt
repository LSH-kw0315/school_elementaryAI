ㅁ머신러닝

ㅇ선형회귀 모델

from sklearn.linear_model import LinearRegression

선형 방정식을 그려서 예측을 하는 방법이다.

객체 내부 변수 : 
model.coef : n차항들의 계수 (n>=1)
model.intercept_ : 상수 (y절편)

tip) x로 들어가는 값이 여러 개, 즉 1개보다 많다면 표준화를 해주는 작업이 필요하다. (모든 요인이 비슷하게 영향을 끼치도록)
이건 선형 회귀건 K 최근접 이웃 모델이건 뭐건 상관 없이 해줘야 하는 일이다. 

tip)pandas로 만든 DataFrame 자료형은 to_numpy() 함수를 사용할 수 있다. 바로 행렬로 바꿔주는 기능이다.

ㅇ예측 근거

predict_proba(테스트케이스) : 왜 이렇게 예측했는지 알 수 있는 함수.
(요인을 직접적으로 알려준다기 보단, 어떤 결과를 얼마만큼의 비중을 두고 추측했는지 알 수 있다.)
ex ) 물고기 종류가 고등어, 참치, 오징어(?)라고 해보자. 
무게, 길이, 높이, 너비를 알려줬을 때 predict는 각 테스트 input에 대해 고등어 or 참치 or 오징어로 예측 결과를 표시해준다.
predict_proba는 각 테스트 input에 대해 [고등어확률, 참치확률, 오징어확률] 이렇게 알려준다.

predict_proba가 숫자의 나열로만 보여서 뭘 근거로 예측했는지 알기 힘들다면 knn.classes_ 속성을 사용하면 알 수 있다.

K 최근접 이웃 모델의 경우 딱히 확률이라고 하기엔 좀 그런 게 보인다. 
(그냥 주변의 데이터를 어느 것을 몇 개를 집어왔는지 알 수 있는 것 뿐이니까.)


ㅇ로지스틱 분류(정확한 확률을 알 수 있는 모델) - 회귀를 이용해 분류하는 모델

일단 선형 방정식을 구하는 건 똑같은데(즉 일단 선형 회귀를 한다는 말이다)
그 선형 방정식을 시그모이드 함수에 집어 넣는다.
시그모이드 함수의 특징은 y값이 0~1까지만 존재한다.

tip ) coef, 즉 계수는 기울기이다. 그리고 머신 러닝에서 기울기를 보통 가중치라고 한다.

ㅇ트리 기반 모델 
1. 대체적으로 성능이 좋은 경향이 있다.
2. 데이터 표준화를 굳이 하지 않아도 된다. (이 모델의 경우, 표준화를 하면 데이터의 설명력이 떨어짐.)
3. 데이터의 설명력이 있다. (즉, 어떤 값 때문에 예측을 이렇게 했는지 알기 쉽다.)
4. 랜덤 포레스트 (이 경우, 데이터 분할을 할 필요가 없음)

*결정 트리 (랜덤 포레스트, XgBoost 가 대표적인 결정 트리 방식에서 파생된 방식)

-랜덤 포레스트는 가볍게 사용하기 좋은 방식이고, XgBoost는 성능이 조금이나마 더 좋은 방식이다.

-K 최근접 이웃 모델처럼 거리 연산을 하거나 선형 방정식을 구하는 등의 연산을 하는 여타 다른 모델과는 달리,

이른바 스무고개 형식으로 답을 예측한다. (무게가 x 이하면 왼쪽, 이상이면 오른쪽... 이런 식으로 계속 간다.)

-라이브러리는 다음과 같다
from sklearn.tree import DecisionTreeClassifier,plot_tree
(plot_tree는 트리를 시각화를 해주는 기능이다)

라이브러리 사용법은 다른 sklearn 라이브러리와 동일하다.

데이터 결정력이 떨어진다는 의미가 무엇인지 표준화한 데이터를 학습시킨 결과를 plot_tree(결정트리,feature_names=,filled=)함수에 넣어보면 알 수 있다.
표준화한 값의 경우 길이 값임에도 음수값이 나와서 기준을 알기가 쉽지 않다.

(하지만 데이터 분할은 필요한 모델이다.)

다만 결정 트리를 사용할 때는 과대적합/과소적합에 주의해야한다.
과대적합이란 훈련데이터를 너무 과하게 학습했을 때를 말한다. (즉, 훈련 데이터에만 적합한 모델을 말한다.)
=>경우의 수를 너무 잘게 쪼갠 경우.
=>과대적합의 대표적인 증상은 훈련데이터 정확도와 시험 데이터의 정확도가 크게 차이나는지 안 나는지 알 수 있다.
(대략 5~10% 이상 차이나면 과대적합이 된 경우라고 볼 수가 있다.)

반대로 과소적합이란 훈련데이터를 너무 적게 학습했을 때를 말한다.
=>대표적 증상 중 하나는 훈련 데이터보다 시험 데이터의 정확도가 더 높게 나오는 경우다.
=>또 다른 증상 중 하나는 훈련 데이터, 시험 데이터 정확도가 둘 다 낮은 경우다.

따라서 너무 깊게, 너무 얕게 학습하지 않도록 최대 깊이를 조절해주어야 한다.

dt=DecisionTreeClassifier(max_depth=)

이런 식으로 조절해주어야 한다.

*랜덤 포레스트

결정 트리 100개를 만들어서 학습하는 모델. 
위의 sklearn의 결정 트리를 써보면 root에 있는 기준이 매번 다른 것을 볼 수 있다.
그것에 착안해서, 여러 기준의 결정 트리를 만들어 숲을 이루는 방법인 셈이다.

또, 결정 트리는 데이터의 모든 특성을 활용한다. (물고기의 무게, 너비, 길이, 높이 특성을 모두 적은 데이터가 있으면, 이 모든 특성을 활용한다.)
하지만 랜덤 포레스트는 휘하 모든 트리가 모든 특성을 활용하지는 않는다. (물고기의 무게, 너비, 길이, 높이 특성 중 2~3개만 가지고 하나의 결정 트리를 만든다. 이런 과정으로 만든 트리가 100개 있다.)
=>데이터의 분할은 딱히 필요가 없다. (하나의 데이터 뭉치를 가지고 특성을 쪼개서 활용하기 때문이다.)
=>또한 이렇게 특성을 제한하는 것은 과대적합을 방어하는 장점도 있다.

강사曰 랜덤 포레스트에 트리가 100개 있으면 어떤 데이터가 들어왔을 때 각각 트리에 데이터를 넣어서 예측해 70개가 도미, 30개가 고등어라면 도미로 예측하는 그런 방식이라고 한다.

ensemble : 여러 개 머신러닝 모델을 결합한 것을 말함

라이브러리는 다음과 같다. from skelearn.ensemble import RandomForestClassifier

그리고 생성자는 다음과 같다.

rf=RandomForestClassifier(oob_score=True,n_estimators=)

oob_score : 선택 못 받은 데이터로 평가할지 말지 <<이건 나중에 다시 영상으로 보도록 하자.(테스트할 데이터가 없을 경우 사용)
n_estimators : 만들 결정 트리의 개수 (디폴트의 개수)

단점이라고 하면 자신이 학습하지 않은 범위의 데이터는 예측을 거의 못한다는 단점이 존재함.
(말그대로 자기가 가진 범위를 가지고 기준을 생성하기 때문)

ex) 100cm 크기의 물고기가 가진 데이터로만 종을 학습했다면 돌연변이로 150cm 크기의 물고기가 가진 데이터로는 종을 알 수가 없음.

크롤링 주의) pandas의 read_html로 테이블 태그를 집어넣을 때는 주의할 것이 있다.
1. html을 직접 집어 넣을 수가 없다. str로 캐스팅해야한다.
2. str(html)로 캐스팅을 해야하지, html.text 이런 식으로 내부의 텍스트를 넘겨주는 게 아님에 유의한다.
3. read_html의 반환 결과물은 리스트다. 따라서, 인자로 하나의 태그만 넘겨줬다면 첫 번째 인덱스의 것을 변수에 할당한다.

tip)데이터프레임의 drop함수의 인자는 행의 index다.
datetime 타입을 sort_value로 정렬하면 가장 오래된 날부터 정렬한다.
데이터프레임의 함수 reset_index를 쓰면 인덱스넘버를 현재 정렬된 순서로 초기화한다.
데이터프레임의 시리즈의 속성에는 dt가 있고, dt에서는 날짜와 관련된 값에 접근할 수가 있다.

주식 분석에서는 회귀 모델을 사용하는 것이 좋다. (평균을 사용함.)

ㅇ케글

케글 : 어떤 회사(혹은 그냥 경진대회 문제)가 직면한 문제를 '인공지능'으로 푸는 일종의 대회

내가 푼 것은 자전거 수요 예측이다.

tip ) 시각화를 이용하면 학습에 어떤 것이 의미있는 데이터인지, 어떤 것이 이상한 데이터인지 알기 쉽다.
이런 쪽으로 나갈 생각이라면 꼭 알아두도록 하자.

seaborn에는 pointplot이 있다. 꺾은선그래프를 그려주는 듯 하다.
seaborn에는 regplot이란 게 있다.

어쨌든 머신 러닝에서 핵심은 데이터 시각화 등을 통해 이상한 데이터를 지워주고, 날짜 데이터처럼 필요한 데이터가 뭉쳐있다면 펼쳐주고 하면서
데이터를 정제해주는 것이다.