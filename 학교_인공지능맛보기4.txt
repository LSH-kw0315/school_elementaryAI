ㅁ데이터 분석-감성 분석

감성 분석이란 부정적이거나 긍정적인 감정이 얼마만큼 글에 담겼는지 분석하는 것이다.

언제나 데이터 분석 tip) 시각화하거나 분석할 때 쓸모없는 데이터를 쳐내는 것은 결과를 보고 해야한다.

random 라이브러리의 random 함수 : 0~1 사이의 임의의 숫자를 반환.
(randint와는 달리 실수 범위임.)

빈도 분석과 유사하지만 다른 점이 있다면 빈도 분석은 유의미한 키워드를 뽑는 것이고(정규표현식으로 유의미한 키워드를 알 수가 있다),

감성 분석은 어떤 감성 분석표를 기반으로 하여 점수를 뽑는다. 이런 식으로..

score_box=[]
for title,desc in zip(df['제목'],df['요약']):
    text=title+" "+desc
    score=0
    for word,polarity in zip(kun['word'],kun['polarity']):
        if word in text:
            score += polarity
    score_box.append(score)

근데 감성 사전의 문제점은 감성의 기준을 어떤 특수한 case에 대해 반영할 수가 없다는 것이다.
어떤 case에 대해선 미쳤다, 돌았다 이런 단어가 긍정적인 감성이지만, 분석표에서는 부정적인 감성으로 적용될 수도 있다는 점이다.
(질병 관련 이슈에서 긍정적인 글임에도 부정적인 감성의 글인 것은 질병 관련이라 그럴 확률이 높다.)


ㅁ머신러닝

tip) 파이썬 객체 자체를 저장할 수가 있다. (dat파일로 저장하는 게 대표적인 듯 하다. 큰 의미는 없지만 데이터를 저장하는 확장자다.)
다만 객체를 저장하거나 불러오려면 pickle 라이브러리가 필요하다.
일단 기본 함수인 open로 파일을 열어야 한다. 단, 옵션은 접미사가 b로 binary를 읽거나 쓰게 할 수 있어야한다.
1. 쓰기 위해서는 pickle.dump(객체, 파일디스크립터) 
2. 읽기 위해서는 loaded_data=pickle.load(파일디스크립터)

tip) 기본 내장 함수인 open 함수는 파일을 열어주는 함수다. 형태는 다음과 같다.
open("파일경로","옵션")
옵션에는 r, rt, ra, w wt, wb, a, at, ab 등이 있다. (t는 text라는 의미다.)

막간의 tmi) bream : 도미, smelt : 빙어 
막간의 tmi) "혼자 공부하는 머신러닝 + 딥러닝", "모두의 딥러닝" 책은 가볍게 공부하기 좋다. (전문적인 서적은 아님.)
"밑바닥부터 시작하는 딥러닝 시리즈" 이 책은 수학적으로 알려준다고 한다.

ㅇK 최근접 이웃 분류 모델

-어떤 미지의 데이터에 대해 가장 가까운 데이터 5개를 가져와서 더 많은 쪽의 데이터로 결정하는 것.
(ex. 미지의 물고기 데이터에 대해서, 5개를 가져와서 4개가 도미고 1개가 빙어라면 이 미지의 물고기 데이터를 도미로 분류한다는 것.)

데이터의 차원이 높아질수록 유용하다. '가장 가까운 데이터'라는 말은 한 점과 한 점 사이 거리를 말한다.

일단 기본적으로는 5개를 가져오지만, 최적의 데이터 개수가 5개인 것은 아니다.

-머신러닝에서는 리스트에 데이터를 저장하면 거리 계산을 못해서 하면 안된다고 한다.

그래서, numpy 라이브러리의 array 함수를 이용해 데이터를 행렬로 바꿔야 학습에 도움되는 데이터를 쌓기 좋다.
(행렬인지 아닌지 구분하는 방법은 array(...) 이런 식으로 데이터가 출력되는지 아닌지 보는 것이다.

-지도학습은 문제집과 정답지를 만들어주어야 하는 방식이다.

문제집은 예를 들자면 수치적인 데이터만 나열된 자료구조만 던져주는 것이라고 할 수가 있겠다.

정답지는 수치적인 데이터가 실제로 무엇인지 알려주는 자료구조라고 할 수가 있겠다.

ex) 문제집은 물고기의 길이와 무게만 있고, 정답지는 대응되는 길이-무게에 대한 물고기의 종류만 있다.

-위 ex대로 만들 경우, 문제지는 2차원 행렬꼴이고 답지는 1차원 행렬꼴이다.

-이런 꼴로 만들어야 머신러닝 라이브러리 중 하나인 sklearn을 통해 K-최근접 이웃 분류 모델로 머신러닝을 할 수가 있다.

-K-최근접 이웃 분류 모델을 위한 sklearn 라이브러리의 import 방법은 아래와 같다:
from sklearn.neighbors import KNeighborsClassifier

-그리고 학습 시키는 코드는 매우 간단하다.
model=KNeighborsClassifier(n_neighbors=)
model.fit(data,target)

여기서 n_neighbors는 몇 개의 이웃한 데이터를 가져올지 결정하는 매개변수다. 

-그 모델로 데이터에 대해 예측을 시키려면 코드는 다음과 같다.

model.predict([ [40,600] ])
(학습 데이터가 2차원 행렬이므로 예측할 것도 2차원 행렬로 넣어줘야한다.)

-data로 학습한 결과와 정답지를 비교하는 방식으로 정확도 계산도 할 수가 있다.
model.score(data,target)

-단, 초기에 학습한 데이터로 평가하면 안된다. 정확도 평가를 하기 위해서는 또 다른 문제집과 정답지를 만들어서 해야한다.
=> 따라서, 데이터를 학습 데이터와 시험 데이터로 나눌 필요가 있다. 보통은 데이터 총량을 기준으로 8:2로 나눈다.
=> 단, 랜덤 함수로 데이터 총량을 가지고 학습데이터 8 : 시험데이터 2 이런 식으로 나누면 학습 모델의 정확도에 문제가 생길 수 있다. (데이터 종류의 비율이 다르면 정확도가 이상해지기 쉽다..)
=> 따라서, 학습 데이터와 시험 데이터 내부의 데이터 종류 비율 또한 동일하게 하여 8:2로 분할해야 한다. ex) 학습 데이터도 도미:빙어 비율이 6:4, 시험 데이터도 도미:빙어 비율이 6:4 여야 한다는 말이다.
=> 이를 위해 데이터 분할 모듈을 사용한다.

데이터 분할 모듈 : from sklearn.model_selection import train_test_split

분할 방법 : train_input, test_input, train_target, test_target =train_test_split(data,target,test_size=0.2)
(순서는 훈련데이터, 테스트데이터, 훈련정답지, 테스트정답지이다. test_size는 0~1 사이의 소수로, 데이터 총량 중 테스트 데이터의 비율이다. 디폴트는 0.25이다.)

-K-최근접 이웃 분류 모델의 경우, 어쨌든 다수결을 통해 데이터를 분류하는 방법이다.
그래서 이웃의 갯수를 이상하게 잡으면 다수결에 의해 데이터 분류가 잘못 될 수가 있다.
(ex. 어느 한 쪽이 15개 밖에 없는 상황에 31개의 이웃 데이터를 가져오도록 모델을 짠다.)

그래서 어떤 모델이 좋은지 알아보려면 반복문 등을 통해 이웃의 수를 바꿔가면서 정확도 평가를 계속 해보는 것이 좋다.

어떤 데이터의 이웃 데이터를 어떻게 뽑아주는지 알고 싶으면...
distance,idx=model.kneighbors([[25,150]]) 
이렇게 쓴다. (가까운 데이터를 직접 알려주는 게 아니라, 가까운 데이터와의 거리, 그리고 데이터의 인덱스를 반환한다.)

-물고기 예제에서 왜 25,150 데이터를 빙어로 받아들였는가? 
=> 사실 그래프를 그릴 때 x축은 눈금당 5만큼 떨어진 거고, y축은 눈금당 200만큼 떨어져있다.
=> 그래서 거리값을 따져볼 때 사실 무게로만 따졌을 때 가까울수록 거리가 가까워질 수밖에 없다
=> 이 모델은 거리로 따지기 때문에 기본적으로 단위가 작은 데이터는 무시되기가 쉽다. 
=> 그래서, 단위가 작은 데이터도 반영할 수 있게 만들어줄 필요가 있다.
=> 이것을 위해서는 통계적 추정을 활용한다. ( x - x의 평균 / x의 표준편차 ) =>표준화 

행렬의 평균, 표준편차를을 구하려면 numpy의 mean, std 함수를 이용한다.
단, 디폴트는 행방향 연산이다. 따라서 axis 옵션을 0으로 하여 열방향 연산을 하게 만들어야한다.

근데 테스트 데이터의 표준화를 거칠 때 테스트 데이터를 토대로 표준화를 하지 않고, 학습 데이터의 평균과 표준편차로 표준화를 한다.
왜냐면, 테스트 데이터는 "알 수 없는 값"이라고 생각하고 컴퓨터는 본인이 학습했던 데이터에 따라 학습하기 때문이다.
(우리가 통계를 배우는 이유는 모집단의 값은 하나도 모르지만 모집단이 대충 어떻게 생겨먹었는지 알기 위해서이다.
그런 논리로 테스트 데이터가 모집단이라고 해봤을 때 우린 모집단이 어떤 형태인지 알리가 없고, 우리가 가진 표본으로만 다뤄야한다는 소리라고 보면 될 거 같다.)

아무튼 이런 방식으로 표준화 하는 것을 standardScaler라고 한다.
(그 외에 RobusterScaler, MinMaxScaler 라는 표준화 방법이 있다.)

tip) sklearn 라이브러리에는 여러 가지 연습용 데이터셋을 제공하고 있다.
from sklearn.datasets import load_iris 예를 들면 이런 것. load_ 시리즈로 보면 많은 것을 볼 수가 있다.

-해당 모델의 단점

사실 엄밀히 따져보면 학습하는 게 아니라 기억해둔 것을 바탕으로 추측을 하는 것이다.
데이터의 양이 많을수록 하나의 데이터가 무엇일 지 예측하는 것이 매우 오래 걸린다는 단점이 있다.

ㅇ표준화방법
-StandardScaler : 평균과 표준편차로 표현 (<=> x - x의 평균 / x의 표준편차 )  - 약점은 이상치에 취약함 
-RobustScaler : 중앙값으로 표준화 - 이상치에 대해 강점을 보임. =>무난하다.
-MinMaxScaler : 최대값을 1, 최소값을 0 (음수가 있는 값이라면 -1 ~ 1 임.) 로 하여 표준화
=> 이상치에 대해 매우매우 취약함. 최대값이 너무 높으면 다른 값들이 전부 최소에 수렴할 가능성이 높음.
단, 이상치가 존재할 리가 없는 이미지의 픽셀값(0~255)에는 자주 쓰인다.

이걸 직접 계산하지 않고 라이브러리를 통해 할 수도 있다.

from sklearn.preprocessing  import StandardScaler, RobustScaler, MinMaxScaler

라이브러리 활용법

ss=StandardScaler()
train_scaled=ss.fit_transform(train_input)

테스트값을 넣기 위해서는 fit_transform을 실행한 이후에 이 코드를 작성하면 된다.
test_scaled=ss.transform(test_input)

(왜냐면, fit_transform을 해버리면 ss가 가진 평균과 표준편차가 들어온 데이터 기준으로 다시 계산되기 때문이다.)

다른 표준화 방법을 사용하려면 생성자만 바꾸면 된다.

ㅇ회귀 모델

-연속적인 값을 예측하는 모델

-회귀 모델에서는 정확도로 성능을 검증하는 게 아니라 얼마나 오차가 있었는지로 검증한다.

-K 최근접 이웃 회귀 모델

어떤 x 값을 던져주면 가까운 y 값 5개 값을 가져와서 그 값으로 평균을 내는 것이다.

이 모델을 쓰려면 이렇게 불러온다.

from sklearn.neighbors import KNeighborsRegressor

용법은 위의 K 최근접 이웃 분류 모델과 유사하다. (그래서 문제집이 2차원이고 정답지도 1차원이어야 하는 것도 똑같다.)

근데 사람 입장에서 보면 문제집도 1차원이고 정답지도 1차원이다. 그래서 이걸 어떻게 하냐면...

1.

data=[]
for i in length
  data.append([i])
data=np.array(data)

본인이 직접 nx1 2차원 배열을 만드는 방법이다.

2.

data = length.reshape(-1,1)
nparray.reshape(분할할 차원) 이렇게 쓴다.
예를 들어 nparray의 원소 수가 20개인데 (4,5) 이렇게 쓴다면..
[
[5개]
[5개]
[5개]
[5개]
]
해서 4x5 행렬을 만들어준다. 주목할 점은, 2차원 배열이라는 점이다.
어느 인수에 -1을 집어넣으면 알아서 공백이 없게 채워지도록 계산한다.
위 예시에서 (2,-1)을 넣으면 2x10 짜리를 알아서 만들고, (-1,2)를 넣으면 10x2 짜리를 알아서 만든다는 의미다.

따라서, 이 방법은 nx1짜리 2차원 배열을 알아서 만드는 방법이다. 

오차를 검증하는 방법은 다음과 같다.

평균 제곱 오차 : MSE (predict - 실제 test 값)^2 의 평균
평균 절대 오차 : MAE (abs(predict - 실제 test 값))의 평균

이 학습모델의 단점은 학습한 데이터의 범위를 벗어나는 것을 물어보면 누가봐도 똑바로 되지 않은 대답을 낸다는 점이다.
(있는 데이터를 기준으로 예측하는 것 뿐이기 때문이다.)
(ex 45cm 물고기까지만 무게가 입력되었다면, 60cm 물고기는 얼마나 무거울지는 알 수가 없다.)

